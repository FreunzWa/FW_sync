###### NLP


# Brief history
    Rule based NLP (hand coded heuristics eg. stemming)
    Statistical NLP (machine learning NLP - automatically learn rules of large corpora)
- 1954 georgetown experiment russian ==> english translation but ultimately unsuccessful after the authors were rogiianly very confident they could solve the issue of machine translation
- 1970s == developed conceptual ontologies which structured real world information into computer undeerstandable data.
- 1980s most NLP built on hand written rules.
- @late 1980s == ML @computing power increase (moores law) 
- supervised/ unsupervised both prominent but unsupervised used more @++data on internet to be used that is not annotated.
- neural machine translation (deep learning approach that obviates need for language pre-modeling that is important in other approaches.)


# Major research goals in NLP
- Syntax goals
    + grammar induction
        * generate formal grammar that describes language syntax
    + lemmatisation
    + morpholobical segmentation
    + part of speech tagging (noun, adj, verb etc.)
    + parsing (determine the parse tree of a given sentence)
    + sentence breaking (most sentences broken @. but . can bee used for other purposes eg @abbrev)
    + stemming
    + terminology extraction
- Semantics
    + machine trasnlation
    + natural language generation
    + natural language understanding (conversion of text into first order logic)
    + question answering
    + sentiment analysis (polarity of opinion on certain subjects)
- Speech
    + eg recognition
- Discourse


# fundamental concepts to current state of NLP
    techniques for vectorising text
- word embeddings
    + where words/phrases from vocabulary are mapped to vectors of real numbers. this mapping can be generated by neuural networks/ probabilistic models. 
    + it aims to categorize semantic similarities between linguistic items (words and phrases). this makes the overall process of NLP much more accurate and robust. there are many ways to say one thing despite superficially the inputs appearing very different. 
    + because ML models use vectors as input, the input text needs to be vectorised. 
    + another important feature is this does not have to be encoded by hand
    + embedding can be likened to using a lookup table.
- one hot encoding (single hot bit, rest are low)
    + eg each word has a differet ID (no significance as to the shape of the encoding). 
    + extremly inefficient @huge number of vecctor elements are 0 'sparse vectors' (unlike the dense vectoors produced in word embeddings)
- uniqe number
    + this is more efficient, however the encoding is arbitrary (again no relationhhip betweeen word)
